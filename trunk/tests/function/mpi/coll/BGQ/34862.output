MPIDI_Process.*
  verbose               : 1
  statistics            : 0
  contexts              : 1
  async_progress        : 0
  context_post          : 0
  short_limit           : 113
  eager_limit           : 2049
  eager_limit_local     : 64
  rma_pending           : 1000
  shmem_pt2pt           : 1
  optimized.collectives : 1
  optimized.select_colls: 2
  optimized.subcomms    : 1
The following MPICH_* environment variables were specified:
The following PAMID_* environment variables were specified:
  PAMID_VERBOSE=1
The following PAMI_* environment variables were specified:
The following COMMAGENT_* environment variables were specified:
The following MUSPI_* environment variables were specified:
The following BG_* environment variables were specified:
  BG_SHAREDMEMSIZE=32
MPI was initialized. 
MPI thread support is THREAD_SINGLE. 
MPI test program running on 2048 ranks. 
MPI_Barrier on MPI_COMM_WORLD 1 
MPI_Comm_dup of MPI_COMM_WORLD 
MPI_Barrier on comm_world_dup 
MPI_Comm_split of MPI_COMM_WORLD into odd-even 
MPI_Barrier on comm_world_oddeven 
MPI_Comm_split MPI_COMM_WORLD into (world-1) 
MPI_Barrier on comm_world_minus_one 
MPI_Comm_group of group_world from MPI_COMM_WORLD 
geomprog_list[0] = 0 
geomprog_list[1] = 1 
geomprog_list[2] = 3 
geomprog_list[3] = 7 
geomprog_list[4] = 15 
geomprog_list[5] = 31 
geomprog_list[6] = 63 
geomprog_list[7] = 127 
geomprog_list[8] = 255 
geomprog_list[9] = 511 
geomprog_list[10] = 1023 
MPI_Group_incl of group_geomprog (geometric progression) from group_world 
MPI_Comm_create of comm_geomprog from group_geomprog on MPI_COMM_WORLD 
MPI_Barrier on comm_geomprog 
MPI_Barrier on MPI_COMM_WORLD 2 
============== BCAST ==============
0: MPI_Bcast 1 integers in 0.000020 seconds (0.204238 MB/s) 
0: MPI_Bcast 2 integers in 0.000010 seconds (0.837039 MB/s) 
0: MPI_Bcast 4 integers in 0.000009 seconds (1.788459 MB/s) 
0: MPI_Bcast 8 integers in 0.000009 seconds (3.581922 MB/s) 
0: MPI_Bcast 16 integers in 0.000009 seconds (7.224496 MB/s) 
0: MPI_Bcast 32 integers in 0.000010 seconds (12.878883 MB/s) 
0: MPI_Bcast 64 integers in 0.000009 seconds (27.515787 MB/s) 
0: MPI_Bcast 128 integers in 0.000010 seconds (53.263979 MB/s) 
0: MPI_Bcast 256 integers in 0.000010 seconds (107.001045 MB/s) 
0: MPI_Bcast 512 integers in 0.000010 seconds (198.281496 MB/s) 
0: MPI_Bcast 1024 integers in 0.000012 seconds (350.647405 MB/s) 
0: MPI_Bcast 2048 integers in 0.000014 seconds (571.418607 MB/s) 
0: MPI_Bcast 4096 integers in 0.000019 seconds (840.312861 MB/s) 
0: MPI_Bcast 8192 integers in 0.000029 seconds (1113.279823 MB/s) 
0: MPI_Bcast 16384 integers in 0.000049 seconds (1329.263222 MB/s) 
0: MPI_Bcast 32768 integers in 0.000086 seconds (1520.755319 MB/s) 
0: MPI_Bcast 65536 integers in 0.000134 seconds (1953.746972 MB/s) 
0: MPI_Bcast 131072 integers in 0.000120 seconds (4376.908628 MB/s) 
0: MPI_Bcast 262144 integers in 0.000198 seconds (5298.079364 MB/s) 
0: MPI_Bcast 524288 integers in 0.000468 seconds (4481.237647 MB/s) 
0: MPI_Bcast 1048576 integers in 0.004088 seconds (1026.007051 MB/s) 
0: MPI_Bcast 2097152 integers in 0.008116 seconds (1033.543434 MB/s) 
0: MPI_Bcast 4194304 integers in 0.015986 seconds (1049.518106 MB/s) 
============== BCAST VS SCATTER+ALLGATHER ==============
0: MPI_Bcast vs MPI_Scatter+MPI_Allgather 2048 integers in 0.000026 vs 0.000728 seconds (313.075049 vs 11.248440 MB/s) 
0: MPI_Bcast vs MPI_Scatter+MPI_Allgather 4096 integers in 0.000020 vs 0.000372 seconds (811.515958 vs 44.016240 MB/s) 
0: MPI_Bcast vs MPI_Scatter+MPI_Allgather 8192 integers in 0.000030 vs 0.000579 seconds (1092.721968 vs 56.575318 MB/s) 
0: MPI_Bcast vs MPI_Scatter+MPI_Allgather 16384 integers in 0.000050 vs 0.000818 seconds (1319.494639 vs 80.136463 MB/s) 
0: MPI_Bcast vs MPI_Scatter+MPI_Allgather 32768 integers in 0.000137 vs 0.001624 seconds (953.823203 vs 80.698862 MB/s) 
0: MPI_Bcast vs MPI_Scatter+MPI_Allgather 65536 integers in 0.000422 vs 0.002934 seconds (621.787368 vs 89.334978 MB/s) 
0: MPI_Bcast vs MPI_Scatter+MPI_Allgather 131072 integers in 0.001185 vs 0.003015 seconds (442.616418 vs 173.901420 MB/s) 
0: MPI_Bcast vs MPI_Scatter+MPI_Allgather 262144 integers in 0.005352 vs 0.007134 seconds (195.920533 vs 146.978753 MB/s) 
0: MPI_Bcast vs MPI_Scatter+MPI_Allgather 524288 integers in 0.010500 vs 0.016760 seconds (199.727739 vs 125.126609 MB/s) 
0: MPI_Bcast vs MPI_Scatter+MPI_Allgather 1048576 integers in 0.021238 vs 0.048042 seconds (197.488676 vs 87.304274 MB/s) 
0: MPI_Bcast vs MPI_Scatter+MPI_Allgather 2097152 integers in 0.029754 vs 0.059572 seconds (281.930191 vs 140.814528 MB/s) 
0: MPI_Bcast vs MPI_Scatter+MPI_Allgather 4194304 integers in 0.102510 vs 0.198974 seconds (163.664979 vs 84.318643 MB/s) 
============== ALLGATHER ==============
0: MPI_Allgather 2048 integers in 0.000225 seconds (36.399182 MB/s) 
0: MPI_Allgather 4096 integers in 0.000253 seconds (64.886486 MB/s) 
0: MPI_Allgather 8192 integers in 0.000253 seconds (129.497313 MB/s) 
0: MPI_Allgather 16384 integers in 0.000421 seconds (155.588001 MB/s) 
0: MPI_Allgather 32768 integers in 0.001628 seconds (80.501352 MB/s) 
0: MPI_Allgather 65536 integers in 0.002645 seconds (99.104064 MB/s) 
0: MPI_Allgather 131072 integers in 0.006388 seconds (82.079414 MB/s) 
0: MPI_Allgather 262144 integers in 0.011032 seconds (95.046411 MB/s) 
0: MPI_Allgather 524288 integers in 0.025275 seconds (82.973939 MB/s) 
0: MPI_Allgather 1048576 integers in 0.054440 seconds (77.044402 MB/s) 
0: MPI_Allgather 2097152 integers in 0.107922 seconds (77.728597 MB/s) 
0: MPI_Allgather 4194304 integers in 0.204117 seconds (82.193993 MB/s) 
============== ALLTOALL ==============
0: MPI_Alltoall 2048 integers in 0.001015 seconds (8.067637 MB/s) 
0: MPI_Alltoall 4096 integers in 0.000970 seconds (16.884434 MB/s) 
0: MPI_Alltoall 8192 integers in 0.000973 seconds (33.690228 MB/s) 
0: MPI_Alltoall 16384 integers in 0.000976 seconds (67.179892 MB/s) 
0: MPI_Alltoall 32768 integers in 0.001224 seconds (107.069113 MB/s) 
0: MPI_Alltoall 65536 integers in 0.001722 seconds (152.188982 MB/s) 
0: MPI_Alltoall 131072 integers in 0.002800 seconds (187.244460 MB/s) 
0: MPI_Alltoall 262144 integers in 0.004967 seconds (211.117017 MB/s) 
0: MPI_Alltoall 524288 integers in 0.009762 seconds (214.825138 MB/s) 
0: MPI_Alltoall 1048576 integers in 0.019334 seconds (216.944720 MB/s) 
0: MPI_Alltoall 2097152 integers in 0.039887 seconds (210.311367 MB/s) 
0: MPI_Alltoall 4194304 integers in 0.077741 seconds (215.809020 MB/s) 
============== REDUCESCATTERBLOCK ==============
MPI_Reduce_scatter_block 2048 integers in 0.000317 seconds (25.881154 MB/s) 
MPI_Reduce_scatter_block 4096 integers in 0.000328 seconds (49.976741 MB/s) 
MPI_Reduce_scatter_block 8192 integers in 0.000377 seconds (86.980354 MB/s) 
MPI_Reduce_scatter_block 16384 integers in 0.000534 seconds (122.649649 MB/s) 
MPI_Reduce_scatter_block 32768 integers in 0.001010 seconds (129.788875 MB/s) 
MPI_Reduce_scatter_block 65536 integers in 0.002128 seconds (123.195062 MB/s) 
MPI_Reduce_scatter_block 131072 integers in 0.032000 seconds (16.383874 MB/s) 
MPI_Reduce_scatter_block 262144 integers in 0.039597 seconds (26.481323 MB/s) 
MPI_Reduce_scatter_block 524288 integers in 0.044913 seconds (46.693660 MB/s) 
MPI_Reduce_scatter_block 1048576 integers in 0.081287 seconds (51.598841 MB/s) 
MPI_Reduce_scatter_block 2097152 integers in 0.149518 seconds (56.104514 MB/s) 
MPI_Reduce_scatter_block 4194304 integers in 0.293614 seconds (57.140351 MB/s) 
