/* -*- Mode: C; c-basic-offset:4 ; -*- */
/*
 *  Copyright (C) 2010 by Argonne National Laboratory.
 *      See COPYRIGHT in top-level directory.
 */

#if !defined OSP_H_INCLUDED
#define OSP_H_INCLUDED

/* Keep C++ compilers from getting confused */
#if defined(__cplusplus)
extern "C"
{
#endif /* __cplusplus */

/** @file osp.h.in */

#include "ospd.h"

/*! \addtogroup osp OSP Public Interface
 * @{
 */

/* ********************************************************************* */
/*                                                                       */
/*               Enumerations                                            */
/*                                                                       */
/* ********************************************************************* */

/**
 * \brief Thread support within OSP.
 *
 * Implementation details:
 *
 * OSP_THREAD_SINGLE, OSP_THREAD_FUNNELED, OSP_THREAD_SERIALIZED and
 * OSP_THREAD_MULTIPLE have the same meaning as the MPI descriptors
 * of similar name.
 *
 * \ingroup ENUMS
 *
 */

typedef enum
{
    OSP_THREAD_SINGLE,     /**< There is only one thread present. */
    OSP_THREAD_FUNNELED,   /**< There are multiple threads present
                                but only the main thread makes OSP calls. */
    OSP_THREAD_SERIALIZED, /**< There are multiple threads present and each can make OSP calls,
                                but never simultaneously. */
    OSP_THREAD_MULTIPLE    /**< There are multiple threads present and each can make OSP calls. */
}
osp_thread_level_t;

/**
 * \brief Datatype support within OSP.
 *
 * \note OSP does not support complex numbers yet.
 *
 * \ingroup ENUMS
 *
 */

typedef enum
{
    OSP_INT32,  /**< int32  */
    OSP_INT64,  /**< int64  */
    OSP_UINT32, /**< uint32 */
    OSP_UINT64, /**< uint64 */
    OSP_FLOAT,  /**< single-precision */
    OSP_DOUBLE, /**< double-precision */
}
osp_datatype_t;

/**
 * \brief OSP boolean.
 *
 * \ingroup ENUMS
 *
 */

typedef enum
{
    OSP_TRUE = 1,
    OSP_FALSE = 0,
}
osp_bool_t;

/**
 * \brief OSP result
 *
 * \ingroup ENUMS
 *
 */

typedef enum
{
    OSP_SUCCESS =  0,
    OSP_FAILURE = -1,
}
osp_result_t;

/* ********************************************************************* */
/*                                                                       */
/*               OSP data structures                                      */
/*                                                                       */
/* ********************************************************************* */

/**
 * \brief OSP offset type.
 *
 * \see OSP_PutMR
 *
 * \ingroup TYPEDEFS
 *
 */

typedef size_t osp_offset_t;

/**
 * \brief OSP global memregion type.
 *
 * A global memregion is like an MPI-2 RMA Window or a stripped-down GA global array.
 * It is an opaque object which holds all the necessary information required to
 * perform RMA operations.
 *
 * The contents of the this type are the local and global info.
 * ospd_local_memregion_info contains either the native aka CPU info,
 * meaning the base pointer and total size, or it contains the
 * device pointer and total size if referring to GPU memory.
 * Obviously, whether the memory is in the same address space
 * or an attached address space must be specified.
 *
 * The global memregion info is where we hide the details of remote registration.
 * For symmetric allocation, it can be O(1) elements whereas the more common
 * case requires O(N) elements.
 *
 * \see OSP_Create_global_memregion, OSP_Allocate_global_memregion, OSP_Destroy_global_memregion
 *
 * \ingroup TYPEDEFS
 *
 */

typedef ospd_window_t osp_window_t;

/**
 * \brief OSP non-blocking handle type.
 *
 * \see OSP_Test_handle, OSP_Test_handle_list,
 *      OSP_Wait_handle, OSP_Wait_handle_list,
 *      OSP_Reset_handle, OSP_Reset_handle_list
 *
 * \ingroup TYPEDEFS
 *
 */

typedef ospd_handle_t osp_handle_t;

/**
 * \brief OSP shared counter.
 *
 * \ingroup TYPEDEFS
 *
 */

typedef ospd_counter_t osp_counter_t;

/**
 * \brief OSP mutex.
 *
 * \ingroup TYPEDEFS
 *
 */

typedef ospd_mutex_t osp_mutex_t;

/**
 * \brief OSP atomic integer type.
 *
 * \ingroup TYPEDEFS
 *
 */

typedef ospd_atomic_t osp_atomic_t;

/**
 * \brief OSP stride descriptor type.
 *
 * \see OSP_PutS, OSP_GetS, OSP_AccS, etc.
 *
 * \ingroup TYPEDEFS
 *
 */

typedef struct osp_iovec_t
{
    int num;
    int offsets[];
    int sizes[];
}
osp_iovec_t;

/* ********************************************************************* */
/*                                                                       */
/*               OSP external API - management                            */
/*                                                                       */
/* ********************************************************************* */

/**
 * \brief Initializes the OSP environment.
 *
 * \note This routine is called OSP_Initialize for syntactic symmetry with
 *       OSP_Finalize and because of Jeff's schadenfreude toward Pavan.
 *
 * \param[out] rc                The error code from initializing OSP
 * \param[in]  OSP_thread_level  The type of thread support for OSP
 *
 * \see OSP_thread_level
 *
 * \ingroup MANAGEMENT
 */

osp_result_t OSP_Initialize(osp_thread_level_t osp_thread_level);

/**
 * \brief Terminates the OSP environment normally.
 *
 * \warning If the MPID device is used, one should be careful in using OSP_Finalize and MPI_Finalize.
 *
 * \note Unlike MPI, OSP_Finalize may be called many times and is thread-safe.
 *       Improper use of OSP_Finalize (calling it more than once) shall be
 *       indicated by a non-zero error code and potentially warnings in
 *       stderr.
 *
 * \note Implementation detail: we need global variable indicated where or not
 *       OSP is alive or not to make this function thread-safe.
 *
 * \param[out] rc    The error code from terminating OSP.
 *
 * \see OSP_Initialize, OSP_Abort
 *
 * \ingroup MANAGEMENT
 */

osp_result_t OSP_Finalize(void);

/**
 * \brief Terminates the OSP environment abnormally.
 *
 * \warning This must be the last OSP call made in any code unless OSP_Finalize is called instead.
 *
 * \param[in]  error_code    The error code to be returned to the submitting environment.
 *                           This is an 8-bit integer because the OS environment will probably truncate it anyways.
 * \param[in]  error_message Text string to print to stderr upon termination.
 *
 * \see OSP_Initialize, OSP_Finalize
 *
 * \ingroup MANAGEMENT
 */

void OSP_Abort(int8_t error_code, char error_message[]);

/* ********************************************************************* */
/*                                                                       */
/*               OSP external API - memory                                */
/*                                                                       */
/* ********************************************************************* */

/**
 * \brief A collective operation to allocate memory to be used in context of OSP operations.
 *
 * \note Memory allocated with this function will be properly aligned for the architecture.
 *
 * \warning Memory allocated with this function must be freed by OSP_Destroy_window.
 *
 * \param[out] rc            The error code.
 * \param[in]  comm          MPI communicator over which the memregion is shared.
 * \param[in]  type          MPI datatype of window
 * \param[in]  count         The number of elements to allocate locally
 * \param[in]  symmetric     Assert that count will be the same on all ranks
 * \param[out] window        Pointer to window to be created.
 *
 * \see OSP_Destroy_window
 *
 * \ingroup MEMORY
 */

osp_result_t OSP_Allocate_window(MPI_Comm           comm,
                                 MPI_Datatype       type,
                                 size_t             count,
                                 osp_bool_t         symmetric,
                                 osp_window_t *     window);

/**
 * \brief A collective operation (over the communicator associated with the window)
 *        to free memory allocated by OSP_Allocate_window.
 *
 * \param[out] rc            The error code.
 * \param[in]  window        Pointer to window to be destroyed.
 *
 * \see OSP_Allocate_window
 *
 * \ingroup MEMORY
 */

osp_result_t OSP_Free_window(osp_window_t * window);

/* ********************************************************************** */
/*                                                                        */
/*               OSP external API - atomic operations                      */
/*                                                                        */
/* ********************************************************************** */

/**
 * \brief Collective operation to allocate and register a counter.
 *        After this operation returns, the shared counter is visible
 *        from every process in the communicator.
 *
 * \param[out]    rc            The error code.
 * \param[in]     comm          MPI communicator over which the counter is shared.
 * \param[out]    atom_size     The size of atomic variable provided (32 or 64)
 * \param[inout]  counter       OSP shared counter.
 *
 * \see osp_counter_t, OSP_Destroy_counter, OSP_Incr_counter, OSP_NbIncr_counter
 *
 * \ingroup Atomics
 */

osp_result_t OSP_Create_counter(MPI_Comm        comm, 
                                size_t        * atom_size,
                                osp_counter_t * counter);

/**
 * \brief Collective operation to deallocate and deregister a counter.
 *
 * \param[out]    rc            The error code.
 * \param[in]     comm          MPI communicator over which the counter is shared.
 * \param[inout]  counter       OSP shared counter.
 *
 * \see osp_counter_t, OSP_Create_counter, OSP_Incr_counter, OSP_NbIncr_counter
 *
 * \ingroup Atomics
 */

osp_result_t OSP_Destroy_counter(osp_counter_t counter);

/**
 * \brief Atomically updates a shared counter and returns the current value.
 *
 * \param[out] rc            The error code.
 * \param[in]  comm          MPI communicator over which the counter is shared.
 * \param[in]  counter       OSP shared counter.
 * \param[in]  increment     The value to add to the counter.
 * \param[in]  original      The remote value of the counter prior to the increment.
 *
 * \see osp_counter_t, OSP_Create_counter, OSP_Destroy_counter, OSP_NbIncr_counter
 *
 * \ingroup Atomics
 */

osp_result_t OSP_Incr_counter(osp_counter_t   counter,
                              osp_atomic_t    increment,
                              osp_atomic_t  * original);

/**
 * \brief Non-blocking atomic RMW update of a shared counter.
 *
 * \param[out] rc            The error code.
 * \param[in]  comm          MPI communicator over which the counter is shared.
 * \param[in]  counter       OSP shared counter.
 * \param[in]  increment     The value to add to the counter.
 * \param[out]  original     The remote value of the counter prior to the increment.
 * \param[out] handle        Opaque handle for the request
 *
 * \see osp_counter_t, OSP_Create_counter, OSP_Destroy_counter, OSP_Incr_counter
 *
 * \ingroup Atomics
 */

osp_result_t OSP_NbIncr_counter(osp_counter_t   counter,
                                osp_atomic_t    increment,
                                osp_atomic_t  * original,
                                osp_handle_t  * handle);

/**
 * \brief Collective operation to initialize a list of mutexes.
 *
 * The allocation of
 *
 * \param[out]    rc            The error code.
 * \param[in]     comm          MPI communicator over which the mutexes are shared.
 * \param[in]     count         Number of mutexes to be created.
 * \param[out]    array         Array storing the number of mutexes on each process
 *
 * \see osp_mutex_t, OSP_Destroy_mutexes, OSP_Lock_mutex, OSP_Trylock_mutex, OSP_Unlock_mutex
 *
 * \ingroup Atomics
 */

osp_result_t OSP_Create_mutexes(MPI_Comm      comm,
                                size_t        count,
                                osp_mutex_t * array[]);

/**
 * \brief Collective operation to uninitialize a list of mutexes
 *
 * \note Implementors: osp_mutex_t cannot be dynamically allocated otherwise this function
 *                     will create a memory leak.
 *
 * \param[out]    rc            The error code.
 * \param[in]     array         Array storing the number of mutexes on each process
 *
 * \see osp_mutex_t, OSP_Create_mutexes, OSP_Lock_mutex, OSP_Trylock_mutex, OSP_Unlock_mutex
 *
 * \ingroup Atomics
 */

osp_result_t OSP_Destroy_mutexes(osp_mutex_t array[]);

/**
 * \brief Local operation to lock a mutex.  This call blocks until the mutex has been locked.
 *
 * \param[out]    rc            The error code.
 * \param[in]     mutex         OSP mutex.
 *
 * \see osp_mutex_t, OSP_Trylock_mutex, OSP_Unlock_mutex, OSP_Create_mutexes, OSP_Destroy_mutex, OSP_Trylock_mutex, OSP_Unlock_mutex
 *
 * \ingroup Atomics
 */

osp_result_t OSP_Lock_mutex(osp_mutex_t  mutex);

/**
 * \brief Local operation to trylock a mutex.
 *
 * \param[out]    rc            The error code.
 * \param[in]     comm          MPI communicator over which the mutexes are shared.
 * \param[in]     mutex         OSP mutex.
 * \param[in]     proc          Process on which you want to lock mutex on
 *
 * \see osp_mutex_t, OSP_Lock_mutex, OSP_Unlock_mutex, OSP_Create_mutexes, OSP_Destroy_mutex, OSP_Trylock_mutex, OSP_Unlock_mutex
 *
 * \ingroup Atomics
 */

osp_result_t OSP_Trylock_mutex(osp_mutex_t  mutex,
                               osp_bool_t * acquired);

/**
 * \brief Local operation to unlock a mutex.  This call blocks until the mutex has been unlocked.
 *
 * \param[out]    rc            The error code.
 * \param[in]     comm          MPI communicator over which the mutexes are shared.
 * \param[in]     mutex         OSP mutex.
 * \param[in]     proc          Process on which you want to lock mutex on
 *
 * \see osp_mutex_t, OSP_Lock_mutex, OSP_Trylock_mutex, OSP_Create_mutexes, OSP_Destroy_mutex, OSP_Trylock_mutex, OSP_Unlock_mutex
 *
 * \ingroup Atomics
 */

osp_result_t OSP_Unlock_mutex(osp_mutex_t mutex);

/**
 * \brief Local operation to trylock a list of mutexes.  This call attempts to acquire all the
 *        locks given and sets the value of the corresponding offset in a vector of booleans
 *        indicating if the lock was acquired or not.
 *
 * \param[out]    rc            The error code.
 * \param[in]     comm          MPI communicator over which the mutexes are shared.
 * \param[in]     count         Number of mutexes to be locked.
 * \param[in]     mutexes       Pointer to vector OSP mutexes.
 * \param[inout]  acquired      Pointer to vector OSP booleans indicating if the lock was acquired.
 *
 * \see osp_mutex_t, OSP_Create_mutexes, OSP_Destroy_mutex, OSP_Trylock_mutex, OSP_Unlock_mutex
 *
 * \ingroup Atomics
 */

osp_result_t OSP_Trylock_mutexes(size_t         count,
                                 osp_mutex_t    mutexes[],
                                 osp_bool_t *   acquired[]);

/**
 * \brief Local operation to unlock a list of mutexes
 *
 * \param[out]    rc            The error code.
 * \param[in]     comm          MPI communicator over which the mutexes are shared.
 * \param[in]     count         Number of mutexes to be locked.
 * \param[in]     mutexes       Pointer to vector OSP mutexes.
 *
 * \see osp_mutex_t, OSP_Create_mutexes, OSP_Destroy_mutex, OSP_Trylock_mutex, OSP_Unlock_mutex
 *
 * \ingroup Atomics
 */

osp_result_t OSP_Unlock_mutexes(size_t        count,
                                osp_mutex_t   mutexes[]);

/* ********************************************************************** */
/*                                                                        */
/*               OSP external API - data transfer operations               */
/*                                                                        */
/* ********************************************************************** */

/**
 * \brief Blocking copy of contiguous data from local memory to remote memory.
 *
 * The mnemonic is PUT BYTES from PTR to WINDOW at RANK plus OFFSET.
 *
 * \param[out] rc            The error code.
 * \param[in]  bytes         Amount of data to transfer in bytes.
 * \param[in]  source_ptr    Starting address in the (local) source memory.
 * \param[in]  window        Window containing target memory.
 * \param[in]  target_rank   Rank of the remote process.
 * \param[in]  target_offset Offset into the memregion at target_rank.

 *
 * \see OSP_NbPut, OSP_MultiPut
 *
 * \ingroup DATA_TRANSFER
 */

osp_result_t OSP_Put(size_t                   bytes,
                     void *                   source_ptr,
                     osp_window_t *           window,
                     int                      target_rank,
                     osp_offset_t             target_offset);

/**
 * \brief Non-Blocking copy of contiguous data from local memory to remote memory.
 *
 * The mnemonic is NBPUT BYTES from PTR to WINDOW at RANK plus OFFSET and get HANDLE back.
 *
 * \param[out] rc            The error code.
 * \param[in]  bytes         Amount of data to transfer in bytes.
 * \param[in]  source_ptr    Starting address in the (local) source memory.
 * \param[in]  window        Window containing target memory.
 * \param[in]  target_rank   Rank of the remote process.
 * \param[in]  target_offset Offset into the memregion at target_rank.
 * \param[out] handle        Opaque handle for the request.
 *
 * \see OSP_NbPut, OSP_MultiPut
 *
 * \ingroup DATA_TRANSFER
 */

osp_result_t OSP_NbPut(size_t                   bytes,
                       void *                   source_ptr,
                       osp_window_t             window,
                       int                      target_rank,
                       osp_offset_t             target_offset,
                       osp_handle_t             handle);

/**
 * \brief Blocking multiple-copy of contiguous data from local memory to remote memory.
 *
 * \param[out] rc            The error code.
 * \param[in]  bytes         Vector of amounts of data to transfer in bytes.
 * \param[in]  source_ptr    Vector of starting addresses in the (local) source memory.
 * \param[in]  memregion     Global memregion containing target memory.
 * \param[in]  target_rank   Vector of ranks of the remote process.
 * \param[in]  target_offset Vector of offsets into the memregion at target_rank.
 *
 * \see OSP_NbPut, OSP_MultiPut
 *
 * \ingroup DATA_TRANSFER
 */

osp_result_t OSP_MultiPut(size_t                   bytes[],
                          void *                   source_ptr[],
                          osp_window_t             window,
                          int                      target_rank[],
                          osp_offset_t             target_offset[]);

/**
 * \brief Non-Blocking multiple-copy of contiguous data from local memory to remote memory.
 *
 * \param[out] rc            The error code.
 * \param[in]  bytes         Vector of amounts of data to transfer in bytes.
 * \param[in]  source_ptr    Vector of starting addresses in the (local) source memory.
 * \param[in]  memregion     Global memregion containing target memory.
 * \param[in]  target_rank   Vector of ranks of the remote process.
 * \param[in]  target_offset Vector of offsets into the memregion at target_rank.
 * \param[out] handle        Opaque handle for the request.
 *
 * \see OSP_NbPut, OSP_MultiPut
 *
 * \ingroup DATA_TRANSFER
 */

osp_result_t OSP_NbMultiPut(int                      bytes[],
                            void *                   source_ptr[],
                            osp_window_t             window,
                            int                      target_rank[],
                            osp_offset_t             target_offset[],
                            osp_handle_t             handle);

/**
 * \brief Blocking copy of strided data from local memory to remote memory.
 *
 * \param[out] rc            The error code.
 * \param[in]  stride_descr  The striding descriptor.
 * \param[in]  source_ptr    Starting address in the (local) source memory.
 * \param[in]  memregion     Global memregion containing target memory.
 * \param[in]  target_rank   Rank of the remote process.
 * \param[in]  target_offset Offset into the memregion at target_rank.
 *
 * \see OSP_Put, OSP_PutV, OSP_MultiPut, OSP_MultiPutS, OSP_MultiPutV
 *
 * \ingroup DATA_TRANSFER
 */

osp_result_t OSP_PutS(osp_stride_descr_t        stride_descr,
                      void *                    source_ptr,
                      osp_window_t              window,
                      int                       target_rank,
                      osp_offset_t              target_offset);

/**
 * \brief Non-Blocking copy of strided data from local memory to remote memory.
 *
 * \param[out] rc              The error code.
 * \param[in]  stride_descr    The striding descriptor.
 * \param[in]  source_ptr      Starting address in the (local) source memory.
 * \param[in]  memregion       Global memregion containing target memory.
 * \param[in]  target_rank     Rank of the remote process.
 * \param[in]  target_offset   Offset into the memregion at target_rank.
 * \param[out] handle          Opaque handle for the request.
 *
 * \see OSP_Put, OSP_PutV, OSP_MultiPut, OSP_MultiPutS, OSP_MultiPutV
 *
 * \ingroup DATA_TRANSFER
 */

osp_result_t OSP_NbPutS(osp_stride_descr_t        stride_descr,
                        void *                    source_ptr,
                        osp_window_t              window,
                        int                       target_rank,
                        osp_offset_t              target_offset,
                        osp_handle_t              handle);

/**
 * \brief Blocking multiple-copy of strided data from local memory to remote memory.
 *
 * \param[out] rc              The error code.
 * \param[in]  stride_descr    The striding descriptor.
 * \param[in]  source_ptr      Starting address in the (local) source memory.
 * \param[in]  memregion       Global memregion containing target memory.
 * \param[in]  target_rank     Rank of the remote process.
 * \param[in]  target_offset   Offset into the memregion at target_rank.
 *
 * \see OSP_Put, OSP_PutV, OSP_MultiPut, OSP_MultiPutS, OSP_MultiPutV
 *
 * \ingroup DATA_TRANSFER
 */

osp_result_t OSP_MultiPutS(osp_stride_descr_t        stride_descr[],
                           void *                    source_ptr[],
                           osp_window_t              window,
                           int                       target_rank[],
                           osp_offset_t              target_offset[]);

/**
 * \brief Non-Blocking multiple-copy of strided data from local memory to remote memory.
 *
 * \param[out] rc              The error code.
 * \param[in]  stride_descr    The striding descriptor.
 * \param[in]  source_ptr      Starting address in the (local) source memory.
 * \param[in]  memregion       Global memregion containing target memory.
 * \param[in]  target_rank     Rank of the remote process.
 * \param[in]  target_offset   Offset into the memregion at target_rank.
 * \param[out] handle          Opaque handle for the request.
 *
 * \see OSP_Put, OSP_PutV, OSP_MultiPut, OSP_MultiPutS, OSP_MultiPutV
 *
 * \ingroup DATA_TRANSFER
 */

osp_result_t OSP_NbMultiPutS(osp_stride_descr_t        stride_descr[],
                             void *                    source_ptr[],
                             osp_window_t              window,
                             int                       target_rank[],
                             osp_offset_t              target_offset[],
                             osp_handle_t              handle);

/* ********************************************************************** */
/*                                                                        */
/*               OSP external API - synchronization and completion         */
/*                                                                        */
/* ********************************************************************** */
/**
 * \brief On return, this call ensures that all blocking and non-blocking put
 *        and accumulate operations sent to the target process have completed remotely.
 *
 *        This operation is local and blocking.
 *
 * \param[out] rc            The error code.
 * \param[in]  proc          Rank of the remote process.
 *
 * \see OSP_NbFlush, OSP_Flush_list
 *
 * \ingroup COMPLETION
 */

osp_result_t OSP_Flush(int proc);

/**
 * \brief When the handle state is OSP_TRUE, all blocking and non-blocking put
 *        and accumulate operations sent to the target process have completed remotely.
 *
 *        This operation is local and non-blocking.
 *
 * \param[out] rc            The error code.
 * \param[in]  proc          Rank of the remote process.
 *
 * \see OSP_Flush, OSP_NbFlush_list
 *
 * \ingroup COMPLETION
 */

osp_result_t OSP_NbFlush(int               proc, 
                         osp_handle_t *    handle);

/**
 * \brief On return, this call ensures that all blocking and non-blocking put
 *        and accumulate operations have completed remotely at the processes given in the list.
 *
 *        This operation is local and blocking.
 *
 * \param[out] rc                The error code.
 * \param[in] count              Number of processes in vector.
 * \param[in] proc_list          Vector of processes to flush against.
 *
 * \see OSP_Flush, OSP_NbFlush_list
 *
 * \ingroup COMPLETION
 */

osp_result_t OSP_Flush_list(size_t         count,
                            int            procs[]);

/**
 * \brief When the handle state is OSP_TRUE, all blocking and non-blocking put
 *        and accumulate operations have completed remotely at the processes given in the list.
 *
 *        This operation is local and blocking.
 *
 * \param[out] rc                The error code.
 * \param[in] count              Number of processes in vector.
 * \param[in] proc_list          Vector of processes to flush against.
 *
 * \see OSP_NbFlush, OSP_Flush_list
 *
 * \ingroup COMPLETION
 */

osp_result_t OSP_NbFlush_list(size_t          count,
                              int             procs[],
                              osp_handle_t *  handle);

/**
 * \brief Blocks on completion of a non-blocking handle.
 *
 * \param[in] handle      Non-blocking handle upon which to be waited.
 *
 * \see osp_handle_t, OSP_Wait_list, OSP_Test
 *
 * \ingroup SYNCHRONIZATION
 */

osp_result_t OSP_Wait(osp_handle_t * handle);

/**
 * \brief Blocks on completion of all elements in a vector of non-blocking handles.
 *
 * \param[in] count        Number of handles in vector.
 * \param[in] handles      Vector of non-blocking handles upon which to be waited.
 *
 * \see osp_handle_t, OSP_Wait, OSP_Test_list
 *
 * \ingroup SYNCHRONIZATION
 */

osp_result_t OSP_Wait_list(size_t             count,
                           osp_handle_t *     handles[]);

/**
 * \brief Blocks on completion of a non-blocking handle.
 *
 * \param[in]  handle        Non-blocking handle upon which to be waited.
 * \param[out] completed     Handle status.
 *
 * \see osp_handle_t, OSP_Wait, OSP_Test_list
 *
 * \ingroup SYNCHRONIZATION
 */

osp_result_t OSP_Test(osp_handle_t *       handle,
                      osp_bool_t *         completed);

/**
 * \brief Blocks on completion of all elements in a vector of non-blocking handles.
 *
 * \param[in]  count        Number of handles in vector.
 * \param[in]  handles      Vector of non-blocking handles upon which to be waited.
 * \param[out] completed    Vector of handle statuses.
 *
 * \see osp_handle_t, OSP_Test, OSP_Wait_list
 *
 * \ingroup SYNCHRONIZATION
 */

osp_result_t OSP_Test_list(size_t               count,
                           osp_handle_t *       handles[],
                           osp_bool_t *         completed[]);

/**
 * \brief Resets a non-blocking handle.
 *
 * \param[in] handle      Non-blocking handle to be reset.
 *
 * \see osp_handle_t, OSP_Reset_handle_list
 *
 * \ingroup SYNCHRONIZATION
 */

osp_result_t OSP_Reset(osp_handle_t * handle);

/**
 * \brief Resets a non-blocking handle.
 *
 * \param[in]  count        Number of handles in vector.
 * \param[in] handle      Non-blocking handle to be reset.
 *
 * \see osp_handle_t, OSP_Reset_handle
 *
 * \ingroup SYNCHRONIZATION
 */

osp_result_t OSP_Reset_list(size_t             count, 
                            osp_handle_t *     handles[]);

/*! @} */

/* ********************************************************************** */
/*                                                                        */
/*               OSP version information                                   */
/*                                                                        */
/* ********************************************************************** */

/* OSP_VERSION is the version string. OSP_NUMVERSION is the
 * numeric version that can be used in numeric comparisons.
 *
 * OSP_VERSION uses the following format:
 * Version: [MAJ].[MIN].[REV][EXT][EXT_NUMBER]
 * Example: 1.0.7rc1 has
 *          MAJ = 1
 *          MIN = 0
 *          REV = 7
 *          EXT = rc
 *          EXT_NUMBER = 1
 *
 * OSP_NUMVERSION will convert EXT to a format number:
 *          ALPHA (a) = 0
 *          BETA (b)  = 1
 *          RC (rc)   = 2
 *          PATCH (p) = 3
 * Regular releases are treated as patch 0
 *
 * Numeric version will have 1 digit for MAJ, 2 digits for MIN, 2
 * digits for REV, 1 digit for EXT and 2 digits for EXT_NUMBER. So,
 * 1.0.7rc1 will have the numeric version 10007201.
 */
#define OSP_VERSION "@OSP_VERSION@"
#define OSP_NUMVERSION @OSP_NUMVERSION@

#define OSP_RELEASE_TYPE_ALPHA  0
#define OSP_RELEASE_TYPE_BETA   1
#define OSP_RELEASE_TYPE_RC     2
#define OSP_RELEASE_TYPE_PATCH  3

#define OSP_CALC_VERSION(MAJOR, MINOR, REVISION, TYPE, PATCH) \
    (((MAJOR) * 10000000) + ((MINOR) * 100000) + ((REVISION) * 1000) + ((TYPE) * 100) + (PATCH))

#if defined(__cplusplus)
}
#endif /* __cplusplus */

#endif /* OSP_H_INCLUDED */
